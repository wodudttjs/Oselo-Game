import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import time

from collections import deque


import logging
import os

def setup_training_logger():
    """Training Pipeline ì „ìš© ë¡œê±° ì„¤ì •"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    logger = logging.getLogger('TrainingPipeline')
    logger.setLevel(logging.INFO)
    
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # íŒŒì¼ í•¸ë“¤ëŸ¬ë§Œ ì¶”ê°€
    file_handler = logging.FileHandler(
        os.path.join(log_dir, 'training_pipeline.log'),
        mode='a',
        encoding='utf-8'
    )
    
    # ë™ì¼í•œ í˜•íƒœì˜ í¬ë§·í„°
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
    )
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    # ì½˜ì†” ì¶œë ¥ ì°¨ë‹¨
    logger.propagate = False
    
    return logger

# ë¡œê±° ì„¤ì •
logger = setup_training_logger()

try:
    from gpu_ultra_strong_ai import UltraStrongAI, GPUSelfPlayTrainer, GPUManager, GPUOthelloNet, BLACK, WHITE
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    logger.warning("GPU ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

class TrainingPipeline:
    """AlphaZero ìŠ¤íƒ€ì¼ í›ˆë ¨ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, model_dir='models', device=None, auto_save_interval=5):
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        else:
            self.device = device
            
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        
        # ìë™ ì €ì¥ ê°„ê²© ì„¤ì •
        self.auto_save_interval = auto_save_interval
        self.game_counter = 0
        self.training_data = deque(maxlen=50000)  # í›ˆë ¨ ë°ì´í„° ë²„í¼
        
        # GPU/CPU íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        self._initialize_trainer()
        
        logger.info(f"í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {self.device})")

    def _initialize_trainer(self):
        """íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”"""
        try:
            if GPU_AVAILABLE and torch.cuda.is_available():
                # GPU íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©
                self.gpu_manager = GPUManager()
                self.neural_net = GPUOthelloNet().to(self.device)
                self.trainer = GPUSelfPlayTrainer(self.neural_net, self.gpu_manager)
                self.gpu_available = True
                logger.info("GPU íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ")
            else:
                raise ImportError("GPU ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€")
                
        except (ImportError, Exception) as e:
            # CPU ë°±ì—… íŠ¸ë ˆì´ë„ˆ
            self.gpu_available = False
            self.neural_net = self._create_simple_neural_net()
            self.optimizer = optim.Adam(self.neural_net.parameters(), lr=0.001)
            self.loss_fn = nn.MSELoss()
            logger.info("CPU íŠ¸ë ˆì´ë„ˆë¡œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _create_simple_neural_net(self):
        """ê°„ë‹¨í•œ ì‹ ê²½ë§ ìƒì„± (GPU ëª¨ë“ˆì´ ì—†ì„ ë•Œ ì‚¬ìš©)"""
        class SimpleOthelloNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
                self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
                self.conv3 = nn.Conv2d(128, 256, 3, padding=1)
                
                # ì •ì±… í—¤ë“œ (64ê°œ ì•¡ì…˜)
                self.policy_head = nn.Sequential(
                    nn.Conv2d(256, 32, 1),
                    nn.ReLU(),
                    nn.Flatten(),
                    nn.Linear(32 * 8 * 8, 64),
                    nn.LogSoftmax(dim=1)  # CrossEntropyLoss ì‚¬ìš©ì„ ìœ„í•´ LogSoftmax ì‚¬ìš©
                )
                
                # ê°€ì¹˜ í—¤ë“œ
                self.value_head = nn.Sequential(
                    nn.Conv2d(256, 16, 1),
                    nn.ReLU(),
                    nn.Flatten(),
                    nn.Linear(16 * 8 * 8, 256),
                    nn.ReLU(),
                    nn.Linear(256, 1),
                    nn.Tanh()
                )

            def forward(self, x):
                x = torch.relu(self.conv1(x))
                x = torch.relu(self.conv2(x))
                x = torch.relu(self.conv3(x))
                
                policy = self.policy_head(x)
                value = self.value_head(x)
                
                return policy, value

        return SimpleOthelloNet().to(self.device)

    def continuous_learning_mode(self):
        """ì—°ì† í•™ìŠµ ëª¨ë“œ ì½œë°± ë°˜í™˜"""
        logger.info("ğŸ“ ì—°ì† í•™ìŠµ ëª¨ë“œ ì‹œì‘ - ê²Œì„í•  ë•Œë§ˆë‹¤ í•™ìŠµí•©ë‹ˆë‹¤")
        
        def post_game_callback(game_data):
            """ê²Œì„ ì™„ë£Œ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°±"""
            try:
                if not game_data:
                    return
                
                # ê²Œì„ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ í›ˆë ¨ ë°ì´í„°ë¡œ ë³€í™˜
                training_data = self.convert_game_data_to_training_data(game_data)
                
                # í›ˆë ¨ ë°ì´í„°ì— ì¶”ê°€
                self.training_data.extend(training_data)
                self.game_counter += 1
                
                # ì¼ì • ê²Œì„ë§ˆë‹¤ í•™ìŠµ ì‹¤í–‰
                if self.game_counter % self.auto_save_interval == 0:
                    logger.info(f"ğŸ“ {self.game_counter}ê²Œì„ ì™„ë£Œ, í•™ìŠµ ì‹œì‘...")
                    
                    # í•™ìŠµ ì‹¤í–‰
                    self._train_neural_network()
                    
                    # ëª¨ë¸ ìë™ ì €ì¥
                    model_path = f"{self.model_dir}/auto_save_{self.game_counter}.pth"
                    self._save_model(model_path)
                    logger.info(f"âœ… ìë™ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {model_path}")
                    
            except Exception as e:
                logger.error(f"âŒ í•™ìŠµ ì½œë°± ì˜¤ë¥˜: {e}")
        
        return post_game_callback

    def _train_neural_network(self):
        """ì‹ ê²½ë§ í›ˆë ¨ ì‹¤í–‰"""
        if len(self.training_data) < 32:
            return
            
        if self.gpu_available and hasattr(self.trainer, 'train_neural_net'):
            # GPU íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©
            self.trainer.train_neural_net(batch_size=32, epochs=2)
        else:
            # CPU íŠ¸ë ˆì´ë„ˆ ì‚¬ìš©
            self._train_cpu_model()

    def _train_cpu_model(self):
        """CPU ëª¨ë¸ í›ˆë ¨"""
        batch_size = 16
        epochs = 2
        
        # í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œë§
        sample_size = min(len(self.training_data), 1000)
        sample_data = list(self.training_data)[-sample_size:]
        
        for epoch in range(epochs):
            total_loss = 0
            batches = 0
            
            for i in range(0, len(sample_data), batch_size):
                batch = sample_data[i:i+batch_size]
                if len(batch) < batch_size:
                    continue
                
                try:
                    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                    boards = torch.stack([item[0] for item in batch]).to(self.device)
                    
                    # ì •ì±… íƒ€ê²Ÿì„ ì›í•« ë²¡í„°ì—ì„œ í´ë˜ìŠ¤ ì¸ë±ìŠ¤ë¡œ ë³€í™˜
                    policy_targets = []
                    for item in batch:
                        action_probs = item[1]
                        if isinstance(action_probs, np.ndarray):
                            target_idx = np.argmax(action_probs)
                        else:
                            target_idx = action_probs
                        policy_targets.append(target_idx)
                    
                    target_policies = torch.tensor(policy_targets, dtype=torch.long).to(self.device)
                    target_values = torch.tensor([item[2] for item in batch], dtype=torch.float32).unsqueeze(1).to(self.device)
                    
                    # ìˆœì „íŒŒ
                    self.optimizer.zero_grad()
                    pred_policies, pred_values = self.neural_net(boards)
                    
                    # ì†ì‹¤ ê³„ì‚°
                    policy_loss = nn.CrossEntropyLoss()(pred_policies, target_policies)
                    value_loss = self.loss_fn(pred_values, target_values)
                    total_loss_batch = policy_loss + value_loss
                    
                    # ì—­ì „íŒŒ
                    total_loss_batch.backward()
                    torch.nn.utils.clip_grad_norm_(self.neural_net.parameters(), 1.0)
                    self.optimizer.step()
                    
                    total_loss += total_loss_batch.item()
                    batches += 1
                    
                except Exception as e:
                    logger.warning(f"ë°°ì¹˜ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            if batches > 0:
                avg_loss = total_loss / batches
                logger.info(f"Epoch {epoch+1}/{epochs}, í‰ê·  ì†ì‹¤: {avg_loss:.4f}")

    def convert_game_data_to_training_data(self, game_data):
        """ê²Œì„ ë°ì´í„°ë¥¼ ì‹ ê²½ë§ í›ˆë ¨ ë°ì´í„°ë¡œ ë³€í™˜"""
        training_data = []
        
        for data in game_data:
            try:
                # ë³´ë“œ ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜
                board_tensor = self.board_to_tensor(data['board'], data['color'])
                
                # ì •ì±… (ì‹¤ì œ ìˆ˜ë¥¼ ì¸ë±ìŠ¤ë¡œ)
                move = data['move']
                move_idx = move[0] * 8 + move[1]
                
                # ê°€ì¹˜ëŠ” ê²Œì„ ê²°ê³¼ ì‚¬ìš©
                value = float(data.get('value', 0.0))
                
                training_data.append((board_tensor, move_idx, value))
                
            except Exception as e:
                logger.warning(f"ë°ì´í„° ë³€í™˜ ì˜¤ë¥˜: {e}")
                continue
        
        return training_data

    def board_to_tensor(self, board_array, color):
        """ë³´ë“œ ë°°ì—´ì„ í…ì„œë¡œ ë³€í™˜"""
        tensor = torch.zeros(3, 8, 8, dtype=torch.float32)
        
        for i in range(8):
            for j in range(8):
                cell_value = board_array[i][j] if isinstance(board_array[i], list) else board_array[i][j]
                
                if cell_value == color:
                    tensor[0][i][j] = 1.0
                elif cell_value != 0:  # ìƒëŒ€ë°© ëŒ
                    tensor[1][i][j] = 1.0
        
        # í˜„ì¬ í”Œë ˆì´ì–´ ì •ë³´
        if color == 1:  # BLACK
            tensor[2] = torch.ones(8, 8)
        
        return tensor

    def _save_model(self, model_path):
        """ëª¨ë¸ ì €ì¥"""
        try:
            if self.gpu_available and hasattr(self.trainer, 'save_model'):
                self.trainer.save_model(model_path)
            else:
                torch.save({
                    'model_state_dict': self.neural_net.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'game_counter': self.game_counter
                }, model_path)
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def load_model(self, model_path):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            if os.path.exists(model_path):
                if self.gpu_available and hasattr(self.trainer, 'load_model'):
                    return self.trainer.load_model(model_path)
                else:
                    checkpoint = torch.load(model_path, map_location=self.device)
                    self.neural_net.load_state_dict(checkpoint['model_state_dict'])
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    self.game_counter = checkpoint.get('game_counter', 0)
                    logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
                    return True
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    pipeline = TrainingPipeline()
    
    # ê¸°ì¡´ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¡œë“œ
    best_model_path = 'models/best_model.pth'
    if os.path.exists(best_model_path):
        logger.info("ê¸°ì¡´ ëª¨ë¸ ë°œê²¬, ë¡œë“œí•©ë‹ˆë‹¤.")
        pipeline.load_model(best_model_path)
    else:
        logger.info("ìƒˆë¡œìš´ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ì—°ì† í•™ìŠµ ëª¨ë“œ ì½œë°± ìƒì„±
    learning_callback = pipeline.continuous_learning_mode()
    
    logger.info("í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    logger.info("GUIì—ì„œ ê²Œì„ì„ ì‹œì‘í•˜ë©´ ìë™ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.")
    
    return pipeline, learning_callback

if __name__ == "__main__":
    main()
